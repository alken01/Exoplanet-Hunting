{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML Project: Exoplanet Hunting in Deep Space**\n",
    "**Alken Rrokaj r0772839, Fatjon Barçi r0732033**\n",
    "\n",
    "### Motivation:\n",
    "Exoplanet hunting in deep space is done by tracking a star over several months or years, to observe if there is a regular 'dimming' of the flux (the light intensity). This is light dimming, is evidence that there may be an orbiting body around the star, such as a planet. This star could be considered to be a 'candidate' system for further depth observations, for example by a satellite that captures light at a different wavelength, could solidify the belief that the candidate can in fact be 'confirmed'. Using a machine learning model is probably the only logical method of making this tedious task possible. \n",
    "\n",
    "### Dataset Description: \n",
    "[Exoplanet Hunting in Deep Space](https://www.kaggle.com/datasets/keplersmachines/kepler-labelled-time-series-data)\n",
    "\n",
    "### Trainset:\n",
    "* 5087 rows or observations.\n",
    "* 3198 columns or features. // too many features. Try downsampling.\n",
    "* Column 1 is the label vector. Columns 2 - 3198 are the flux values over time.\n",
    "* 37 confirmed exoplanet-stars and 5050 non-exoplanet-stars.\n",
    "\n",
    "### Testset:\n",
    "* 570 rows or observations.\n",
    "* 3198 columns or features.\n",
    "* Column 1 is the label vector. Columns 2 - 3198 are the flux values over time.\n",
    "* 5 confirmed exoplanet-stars and 565 non-exoplanet-stars. -->\n",
    "<!-- \n",
    "### References:\n",
    "Let’s find planets beyond our solar system & milky way … . Available at: https://medium.datadriveninvestor.com/lets-find-planets-beyond-our-solar-system-milky-way-galaxy-with-the-help-of-905dcfc95d3d (Accessed: November 7, 2022). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train data\n",
    "train_df = pd.read_csv('./exoTrain.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Analysis**\n",
    "Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features per star measurement\n",
    "train_df.T.describe().T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the data in two lists, based on whether they have exoplanets or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exoplanets = []\n",
    "no_exoplanets = []\n",
    "for i in range(len(train_df)):\n",
    "    if train_df['LABEL'][i] == 2:\n",
    "        exoplanets.append(np.array(train_df.iloc[i,1:]))\n",
    "    else:\n",
    "        no_exoplanets.append(np.array(train_df.iloc[i,1:]))\n",
    "        \n",
    "no_exoplanets = np.array(no_exoplanets[1:])\n",
    "exoplanets = np.array(exoplanets[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('no_exoplanets')\n",
    "pd.DataFrame(no_exoplanets).T.describe().T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('exoplanets')\n",
    "pd.DataFrame(exoplanets).T.describe().T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTheData(data_name, title, ylimit, xlimit, rang=10):\n",
    "    plt.figure(figsize=(15,5))  \n",
    "    if rang == 0:\n",
    "        rang = len(data_name)-1\n",
    "    for i in range(0,rang):\n",
    "        plt.plot(data_name[i])\n",
    "        if ylimit != 0:\n",
    "            plt.ylim([-1*ylimit,ylimit])\n",
    "        if xlimit != 0:\n",
    "            plt.xlim(0,xlimit);\n",
    "    plt.title(label=title)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTheData(exoplanets,'Exoplanet Stars\\' Light Intensity vs Time', 5000,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTheData(no_exoplanets,'No Exoplanet Stars\\' Light Intensity vs Time', 5000,0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalize the data in based on max intensity of each star, this would make the absolute magnitude more similar with the other stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalizeArray(X_array):\n",
    "    normalized_array = []\n",
    "    for X in X_array:\n",
    "        normalized_array.append(featureNormalize(X))\n",
    "    return np.array(normalized_array)\n",
    "        \n",
    "# Code Assignment starts here\n",
    "# EX1. Optional Exercises: \n",
    "# 3.1 Feature Normalization     \n",
    "def featureNormalize(X):\n",
    "    \"\"\"\n",
    "    Normalizes the features in X. returns a normalized version of X where\n",
    "    the mean value of  xeach feature is 0 and the standard deviation\n",
    "    is 1. This is often a good preprocessing step to do when working with\n",
    "    learning algorithms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_norm : array_like\n",
    "        The normalized dataset of shape (m x n).\n",
    "    \"\"\"\n",
    "    # You need to set these values correctly\n",
    "    X_norm = X.copy()\n",
    "    mu = np.zeros(X.shape)\n",
    "    sigma = np.zeros(X.shape)\n",
    "\n",
    "    # =========================== YOUR CODE HERE =====================\n",
    "    mu = np.mean(X, axis=0)\n",
    "    X_norm = X - mu\n",
    "\n",
    "    sigma = np.std(X_norm, axis=0, ddof=1)\n",
    "    X_norm /= sigma\n",
    "    # ================================================================\n",
    "    return X_norm\n",
    "# Code Assignment stops here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_exo = featureNormalizeArray(exoplanets)\n",
    "normalized_no_exo = featureNormalizeArray(no_exoplanets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones(len(normalized_no_exo), dtype=int)\n",
    "twos = np.full(len(normalized_exo),2,dtype=int)\n",
    "\n",
    "first = pd.DataFrame(normalized_exo)\n",
    "first.insert(0,'LABEL',twos)\n",
    "\n",
    "second = pd.DataFrame(normalized_no_exo)\n",
    "second.insert(0,'LABEL',ones)      \n",
    "\n",
    "normalized_whole = pd.concat([first,second],axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTheData(normalized_whole,'Normalized Exoplanet Stars\\' Light Intensity vs Time',20,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTheData(normalized_exo,'Normalized Exoplanet Stars\\' Light Intensity vs Time',20,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTheData(normalized_no_exo,'Normalized No Exoplanet Stars\\' Light Intensity vs Time',20,0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Balancing**\n",
    "In this part of code we balance the classes using the SMOTE technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE (random_state = 69) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we separate the data and the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalized_whole.drop(['LABEL'], axis=1)\n",
    "y_train = normalized_whole['LABEL']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code applies the SMOTE oversampling technique to the training data, which is stored in x_train and y_train. The fit_sample method fits the SMOTE model to the training data and generates synthetic minority class samples, which are then combined with the original training data. The resulting oversampled dataset is stored in x_train and y_train. Finally the oversampled data is split into a new training and test set considering that the test set is also very imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = smote.fit_resample (x_train, y_train.ravel ())\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.3, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code assignment starts here\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        The input to the sigmoid function. This can be a 1-D vector \n",
    "        or a 2-D matrix. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "    \"\"\"    \n",
    "    z = np.array(z)\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    # =============================================================\n",
    "    return g\n",
    "\n",
    "# Code assignment ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code assignment starts here\n",
    "def costFunction(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        The parameters for logistic regression. This a vector\n",
    "        of shape (n+1, ).\n",
    "    \n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n+1) where m is the total number\n",
    "        of data points and n is the number of features. We assume the \n",
    "        intercept has already been added to the input.\n",
    "    \n",
    "    y : arra_like\n",
    "        Labels for the input. This is a vector of shape (m, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        A vector of shape (n+1, ) which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "    \n",
    "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
    "    grad = (1 / m) * (h - y).dot(X)\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    return J, grad\n",
    "# Code assignment ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code assignment starts here\n",
    "def predict(theta, X):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression.\n",
    "    Computes the predictions for X using a threshold at 0.5 \n",
    "    (i.e., if sigmoid(theta.T*x) >= 0.5, predict 1)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        Parameters for logistic regression. A vecotor of shape (n+1, ).\n",
    "    \n",
    "    X : array_like\n",
    "        The data to use for computing predictions. The rows is the number \n",
    "        of points to compute predictions, and columns is the number of\n",
    "        features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : array_like\n",
    "        Predictions and 0 or 1 for each row in X. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the following code to make predictions using your learned \n",
    "    logistic regression parameters.You should set p to a vector of 0's and 1's    \n",
    "    \"\"\"\n",
    "    m = X.shape[0] # Number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    p = np.zeros(m)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    p = np.round(sigmoid(X.dot(theta.T)))\n",
    "    \n",
    "    # ===Adjusment to code since our labels are 1 2 and not 0 1 ==\n",
    "    \n",
    "    p = [x+1 for x in p]\n",
    "    \n",
    "    # ============================================================\n",
    "    \n",
    "    return p\n",
    "# Code assignment ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./exoTest.csv')\n",
    "df_test_x = test_df.drop(['LABEL'], axis=1)\n",
    "df_test_y = test_df['LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n = x_test.shape \n",
    "initial_theta = np.zeros(n)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_theta,\n",
    "                        (x_train, y_train),\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options={'maxiter': 400})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = res.x\n",
    "results = predict(theta,df_test_x)\n",
    "accuracy = accuracy_score(df_test_y, results)\n",
    "print(\"The accuracy of the model is: \" + str(accuracy))\n",
    "conf = confusion_matrix(df_test_y, results)\n",
    "print(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
